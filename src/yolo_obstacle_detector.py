#!/usr/bin/env python3
"""
YOLOv8åˆ†å‰²éšœç¢ç‰©æ£€æµ‹æ¨¡å—
ä½¿ç”¨YOLOv8-segè¿›è¡Œè¯­ä¹‰åˆ†å‰²ï¼Œè¾…åŠ©ç”Ÿæˆéšœç¢ç‰©æ©è†œ
"""

import cv2 as cv
import numpy as np
from typing import List, Tuple, Optional, Dict, Any
import time
import os
import torch

try:
    from ultralytics import YOLO
    YOLO_AVAILABLE = True
except ImportError:
    YOLO_AVAILABLE = False
    print("âš ï¸ ultralytics not available, YOLOv8-segåŠŸèƒ½ä¸å¯ç”¨")

try:
    import onnxruntime as ort
    ONNX_AVAILABLE = True
except ImportError:
    ONNX_AVAILABLE = False
    print("âš ï¸ onnxruntime not available, ONNXæ¨¡å‹åŠŸèƒ½ä¸å¯ç”¨")

class YOLOObstacleDetector:
    """
    YOLOv8åˆ†å‰²éšœç¢ç‰©æ£€æµ‹å™¨
    ä½¿ç”¨YOLOv8-segè¿›è¡Œè¯­ä¹‰åˆ†å‰²ï¼Œè¯†åˆ«å’Œåˆ†ç¦»éšœç¢ç‰©
    """
    
    def __init__(self, 
                 model_path: str = "yolov8n-seg.pt",
                 confidence_threshold: float = 0.5,
                 device: str = "auto",
                 use_quantized: bool = True,
                 quantization_type: str = "fp16"):
        """
        åˆå§‹åŒ–YOLOv8éšœç¢ç‰©æ£€æµ‹å™¨
        
        Args:
            model_path: YOLOv8åˆ†å‰²æ¨¡å‹è·¯å¾„
            confidence_threshold: æ£€æµ‹ç½®ä¿¡åº¦é˜ˆå€¼
            device: è¿è¡Œè®¾å¤‡ ("cpu", "cuda", "auto")
            use_quantized: æ˜¯å¦ä½¿ç”¨é‡åŒ–æ¨¡å‹
            quantization_type: é‡åŒ–ç±»å‹ ("fp16", "int8", "original")
        """
        self.model_path = model_path
        self.confidence_threshold = confidence_threshold
        self.device = device
        self.use_quantized = use_quantized
        self.quantization_type = quantization_type
        
        # æ ¹æ®é‡åŒ–ç±»å‹é€‰æ‹©æ¨¡å‹è·¯å¾„
        if use_quantized:
            self.model_path = self._get_quantized_model_path(quantization_type)
        
        # éšœç¢ç‰©ç±»åˆ«ï¼ˆä½¿ç”¨YOLOv8æ¨¡å‹è‡ªå¸¦çš„ç±»åˆ«åç§°ï¼‰
        # è¿™äº›æ˜¯COCOæ•°æ®é›†çš„80ä¸ªç±»åˆ«ï¼ŒYOLOv8ä¼šè‡ªåŠ¨è¯†åˆ«
        self.obstacle_class_names = {
            # å®¶å…·ç±»
            'chair', 'couch', 'bed', 'dining table', 'toilet',
            'tv', 'laptop', 'mouse', 'remote', 'keyboard',
            'cell phone', 'microwave', 'oven', 'toaster',
            'sink', 'refrigerator', 'book', 'clock', 'vase',
            'scissors', 'teddy bear', 'hair drier', 'toothbrush',
            
            # äº¤é€šå·¥å…·ç±»
            'car', 'motorcycle', 'airplane', 'bus', 'train',
            'truck', 'boat', 'bicycle',
            
            # å…¶ä»–ç‰©ä½“
            'bottle', 'wine glass', 'cup', 'fork', 'knife',
            'spoon', 'bowl', 'banana', 'apple', 'sandwich',
            'orange', 'broccoli', 'carrot', 'hot dog',
            'pizza', 'donut', 'cake',
            
            # è¿åŠ¨ç”¨å“
            'sports ball', 'tennis racket',
        }
        
        # æ‰‹éƒ¨ç›¸å…³ç±»åˆ«ï¼ˆéœ€è¦æ’é™¤ï¼‰
        self.hand_related_class_names = {
            'person',  # äººä½“ï¼ŒåŒ…å«æ‰‹éƒ¨
        }
        
        self.model = None
        self.is_initialized = False
        
        # æ€§èƒ½ç»Ÿè®¡
        self.inference_times = []
        
        # åˆå§‹åŒ–æ¨¡å‹
        self._initialize_model()
    
    def _get_quantized_model_path(self, quantization_type: str) -> str:
        """
        æ ¹æ®é‡åŒ–ç±»å‹è·å–å¯¹åº”çš„æ¨¡å‹è·¯å¾„
        
        Args:
            quantization_type: é‡åŒ–ç±»å‹
            
        Returns:
            é‡åŒ–æ¨¡å‹è·¯å¾„
        """
        quantized_models_dir = "src/quantized_models"
        
        if quantization_type == "fp16":
            # ä¼˜å…ˆé€‰æ‹©ONNXæ ¼å¼ï¼Œå…¶æ¬¡OpenVINO
            onnx_path = os.path.join(quantized_models_dir, "yolov8n-seg_fp16.onnx")
            onnx_fallback = "src/yolov8n-seg.onnx"  # å¯¼å‡ºçš„ONNXæ–‡ä»¶
            openvino_path = os.path.join(quantized_models_dir, "yolov8n-seg_fp16")
            
            if os.path.exists(onnx_path):
                return onnx_path
            elif os.path.exists(onnx_fallback):
                print(f"ğŸ“¦ ä½¿ç”¨å¯¼å‡ºçš„ONNXæ¨¡å‹: {onnx_fallback}")
                return onnx_fallback
            elif os.path.exists(openvino_path):
                return openvino_path
            else:
                print(f"âš ï¸  FP16é‡åŒ–æ¨¡å‹ä¸å­˜åœ¨ï¼Œä½¿ç”¨åŸå§‹æ¨¡å‹")
                return "yolov8n-seg.pt"
                
        elif quantization_type == "int8":
            # ä¼˜å…ˆé€‰æ‹©ONNXæ ¼å¼ï¼Œå…¶æ¬¡OpenVINO
            onnx_path = os.path.join(quantized_models_dir, "yolov8n-seg_int8.onnx")
            openvino_path = os.path.join(quantized_models_dir, "yolov8n-seg_int8")
            
            if os.path.exists(onnx_path):
                return onnx_path
            elif os.path.exists(openvino_path):
                return openvino_path
            else:
                print(f"âš ï¸  INT8é‡åŒ–æ¨¡å‹ä¸å­˜åœ¨ï¼Œä½¿ç”¨FP16æ¨¡å‹")
                return self._get_quantized_model_path("fp16")
                
        else:
            return "yolov8n-seg.pt"
    
    def _initialize_model(self):
        """åˆå§‹åŒ–YOLOv8æ¨¡å‹"""
        try:
            if not YOLO_AVAILABLE:
                print("âŒ ultralyticsä¸å¯ç”¨ï¼Œæ— æ³•åˆå§‹åŒ–YOLOv8æ¨¡å‹")
                return
            
            print(f"ğŸ”„ åŠ è½½YOLOv8æ¨¡å‹: {self.model_path}")
            print(f"   é‡åŒ–ç±»å‹: {self.quantization_type}")
            print(f"   è®¾å¤‡: {self.device}")
            
            # åŠ è½½æ¨¡å‹ï¼ˆä¿®å¤PyTorch 2.6çš„weights_onlyé—®é¢˜ï¼‰
            try:
                self.model = YOLO(self.model_path)
            except Exception as e:
                if "weights_only" in str(e):
                    # å¯¹äº.ptæ–‡ä»¶ï¼Œä½¿ç”¨weights_only=False
                    import torch
                    torch.serialization.add_safe_globals(['ultralytics.nn.tasks.SegmentationModel'])
                    self.model = YOLO(self.model_path)
                else:
                    raise e
            
            # è®¾ç½®è®¾å¤‡
            if self.device == "auto":
                self.device = "cuda" if torch.cuda.is_available() else "cpu"
            
            self.is_initialized = True
            print(f"âœ… YOLOv8æ¨¡å‹åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ YOLOv8æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.is_initialized = False
            print("âŒ YOLOv8ä¸å¯ç”¨ï¼Œè¯·å®‰è£…ultralytics: pip install ultralytics")
    
    def get_model_info(self) -> Dict[str, Any]:
        """
        è·å–æ¨¡å‹ä¿¡æ¯
        
        Returns:
            æ¨¡å‹ä¿¡æ¯å­—å…¸
        """
        if not self.is_initialized:
            return {"error": "æ¨¡å‹æœªåˆå§‹åŒ–"}
        
        info = {
            "model_path": self.model_path,
            "quantization_type": self.quantization_type,
            "use_quantized": self.use_quantized,
            "device": self.device,
            "confidence_threshold": self.confidence_threshold,
            "model_size": self._get_model_size(),
            "average_inference_time": np.mean(self.inference_times) if self.inference_times else 0,
            "fps": 1.0 / np.mean(self.inference_times) if self.inference_times else 0
        }
        
        return info
    
    def _get_model_size(self) -> str:
        """è·å–æ¨¡å‹æ–‡ä»¶å¤§å°"""
        try:
            if os.path.exists(self.model_path):
                size_bytes = os.path.getsize(self.model_path)
                if size_bytes < 1024 * 1024:
                    return f"{size_bytes / 1024:.1f} KB"
                else:
                    return f"{size_bytes / (1024 * 1024):.1f} MB"
            else:
                return "Unknown"
        except:
            return "Unknown"
    
    def benchmark_performance(self, test_image: np.ndarray, num_runs: int = 20) -> Dict[str, float]:
        """
        æ€§èƒ½åŸºå‡†æµ‹è¯•
        
        Args:
            test_image: æµ‹è¯•å›¾åƒ
            num_runs: æµ‹è¯•æ¬¡æ•°
            
        Returns:
            æ€§èƒ½æµ‹è¯•ç»“æœ
        """
        if not self.is_initialized:
            return {"error": "æ¨¡å‹æœªåˆå§‹åŒ–"}
        
        print(f"ğŸ”„ å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯• ({num_runs}æ¬¡è¿è¡Œ)...")
        
        # é¢„çƒ­
        for _ in range(5):
            _ = self.model(test_image, verbose=False)
        
        # æ€§èƒ½æµ‹è¯•
        times = []
        for i in range(num_runs):
            start_time = time.time()
            _ = self.model(test_image, verbose=False)
            inference_time = time.time() - start_time
            times.append(inference_time)
            
            if (i + 1) % 5 == 0:
                print(f"   å®Œæˆ {i + 1}/{num_runs} æ¬¡æµ‹è¯•")
        
        avg_time = np.mean(times)
        std_time = np.std(times)
        fps = 1.0 / avg_time
        
        results = {
            "average_time": avg_time,
            "std_time": std_time,
            "fps": fps,
            "min_time": np.min(times),
            "max_time": np.max(times)
        }
        
        print(f"âœ… æ€§èƒ½æµ‹è¯•å®Œæˆ:")
        print(f"   å¹³å‡æ¨ç†æ—¶é—´: {avg_time:.4f}s Â± {std_time:.4f}s")
        print(f"   FPS: {fps:.2f}")
        print(f"   æœ€å°/æœ€å¤§æ—¶é—´: {np.min(times):.4f}s / {np.max(times):.4f}s")
        
        return results
    
    def _initialize_model(self):
        """
        åˆå§‹åŒ–YOLOv8æ¨¡å‹ï¼ˆæ”¯æŒPyTorchå’ŒONNXæ ¼å¼ï¼‰
        """
        try:
            print(f"ğŸ”„ åŠ è½½YOLOv8åˆ†å‰²æ¨¡å‹: {self.model_path}")
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºONNXæ¨¡å‹
            if self.model_path.endswith('.onnx'):
                if not ONNX_AVAILABLE:
                    print("âŒ ONNX Runtimeä¸å¯ç”¨ï¼Œæ— æ³•åŠ è½½ONNXæ¨¡å‹")
                    self.is_initialized = False
                    return
                
                # åŠ è½½ONNXæ¨¡å‹
                self.model = self._load_onnx_model()
                self.model_type = "onnx"
                
            else:
                # åŠ è½½PyTorchæ¨¡å‹
                self.model = YOLO(self.model_path)
                self.model_type = "pytorch"
                
                # è®¾ç½®è®¾å¤‡
                if self.device == "auto":
                    self.device = "cuda" if self.model.device.type == "cuda" else "cpu"
            
            print(f"âœ… YOLOv8æ¨¡å‹åŠ è½½æˆåŠŸ ({self.model_type})")
            print(f"   è®¾å¤‡: {self.device}")
            print(f"   ç½®ä¿¡åº¦é˜ˆå€¼: {self.confidence_threshold}")
            print(f"   éšœç¢ç‰©ç±»åˆ«æ•°: {len(self.obstacle_class_names)}")
            
            if self.model_type == "pytorch":
                print(f"   æ¨¡å‹è‡ªå¸¦ç±»åˆ«: {len(self.model.names)} ä¸ª")
                print(f"   æ¨¡å‹æ”¯æŒçš„ç±»åˆ«: {list(self.model.names.values())}")
            
            self.is_initialized = True
            
        except Exception as e:
            print(f"âŒ YOLOv8æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            self.is_initialized = False
    
    def _load_onnx_model(self):
        """
        åŠ è½½ONNXæ¨¡å‹
        """
        try:
            # è®¾ç½®ONNX Runtimeæä¾›è€…
            providers = ['CPUExecutionProvider']
            if self.device == "cuda":
                providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']
            
            # åˆ›å»ºONNX Runtimeä¼šè¯
            session = ort.InferenceSession(self.model_path, providers=providers)
            
            # è·å–è¾“å…¥è¾“å‡ºä¿¡æ¯
            input_info = session.get_inputs()[0]
            output_info = session.get_outputs()
            
            print(f"   ONNXæ¨¡å‹è¾“å…¥: {input_info.name}, å½¢çŠ¶: {input_info.shape}")
            print(f"   ONNXæ¨¡å‹è¾“å‡ºæ•°é‡: {len(output_info)}")
            
            # åˆ›å»ºæ¨¡å‹åŒ…è£…å™¨
            model_wrapper = {
                'session': session,
                'input_name': input_info.name,
                'input_shape': input_info.shape,
                'output_names': [output.name for output in output_info],
                'names': {i: f'class_{i}' for i in range(80)}  # COCOæ•°æ®é›†80ä¸ªç±»åˆ«
            }
            
            return model_wrapper
            
        except Exception as e:
            print(f"âŒ ONNXæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise e
    
    def _run_onnx_inference(self, image: np.ndarray):
        """
        è¿è¡ŒONNXæ¨¡å‹æ¨ç†
        """
        try:
            # é¢„å¤„ç†å›¾åƒ
            input_tensor = self._preprocess_image_for_onnx(image)
            
            # è¿è¡Œæ¨ç†
            outputs = self.model['session'].run(
                self.model['output_names'], 
                {self.model['input_name']: input_tensor}
            )
            
            # åå¤„ç†ç»“æœ
            results = self._postprocess_onnx_outputs(outputs, image.shape)
            
            return results
            
        except Exception as e:
            print(f"âŒ ONNXæ¨ç†å¤±è´¥: {e}")
            return None
    
    def _preprocess_image_for_onnx(self, image: np.ndarray) -> np.ndarray:
        """
        ä¸ºONNXæ¨¡å‹é¢„å¤„ç†å›¾åƒ
        """
        # è°ƒæ•´å›¾åƒå¤§å°åˆ°æ¨¡å‹è¾“å…¥å°ºå¯¸
        input_size = 640  # YOLOv8é»˜è®¤è¾“å…¥å°ºå¯¸
        resized = cv.resize(image, (input_size, input_size))
        
        # è½¬æ¢ä¸ºRGB
        rgb = cv.cvtColor(resized, cv.COLOR_BGR2RGB)
        
        # å½’ä¸€åŒ–åˆ°[0,1]
        normalized = rgb.astype(np.float32) / 255.0
        
        # è½¬æ¢ä¸ºCHWæ ¼å¼å¹¶æ·»åŠ batchç»´åº¦
        input_tensor = np.transpose(normalized, (2, 0, 1))
        input_tensor = np.expand_dims(input_tensor, axis=0)
        
        return input_tensor
    
    def _postprocess_onnx_outputs(self, outputs, original_shape):
        """
        åå¤„ç†ONNXæ¨¡å‹è¾“å‡º
        """
        # è¿™é‡Œéœ€è¦æ ¹æ®YOLOv8-segçš„ONNXè¾“å‡ºæ ¼å¼è¿›è¡Œåå¤„ç†
        # ç”±äºONNXè¾“å‡ºæ ¼å¼å¤æ‚ï¼Œè¿™é‡Œå…ˆè¿”å›ä¸€ä¸ªç®€å•çš„åŒ…è£…å™¨
        # å®é™…åº”ç”¨ä¸­éœ€è¦æ ¹æ®å…·ä½“çš„ONNXæ¨¡å‹è¾“å‡ºæ ¼å¼è¿›è¡Œè§£æ
        
        class ONNXResult:
            def __init__(self, outputs, original_shape):
                self.outputs = outputs
                self.original_shape = original_shape
                self.masks = None  # åˆ†å‰²æ©è†œ
                self.boxes = None  # è¾¹ç•Œæ¡†
                self.names = {i: f'class_{i}' for i in range(80)}
            
            def __iter__(self):
                return iter([self])
        
        return ONNXResult(outputs, original_shape)
    
    def detect_obstacles(self, 
                        image: np.ndarray, 
                        hand_landmarks_3d: Optional[List] = None) -> Dict[str, Any]:
        """
        æ£€æµ‹å›¾åƒä¸­çš„éšœç¢ç‰©
        
        Args:
            image: è¾“å…¥å›¾åƒ (BGRæ ¼å¼)
            hand_landmarks_3d: æ‰‹éƒ¨å…³é”®ç‚¹3Dåæ ‡åˆ—è¡¨
            
        Returns:
            Dict: æ£€æµ‹ç»“æœ
        """
        if not self.is_initialized:
            return self._get_empty_result()
        
        start_time = time.time()
        
        try:
            # æ ¹æ®æ¨¡å‹ç±»å‹è¿›è¡Œæ¨ç†
            if self.model_type == "onnx":
                results = self._run_onnx_inference(image)
            else:
                # PyTorchæ¨¡å‹æ¨ç†
                results = self.model(image, 
                                   conf=self.confidence_threshold,
                                   device=self.device,
                                   verbose=False)
            
            inference_time = time.time() - start_time
            self.inference_times.append(inference_time)
            
            # ä¿æŒæœ€è¿‘100æ¬¡çš„æ¨ç†æ—¶é—´
            if len(self.inference_times) > 100:
                self.inference_times = self.inference_times[-100:]
            
            # å¤„ç†æ£€æµ‹ç»“æœ
            detection_result = self._process_detection_results(results[0], image.shape)
            
            # ç”Ÿæˆéšœç¢ç‰©æ©è†œ
            obstacle_mask = self._generate_obstacle_mask(
                detection_result, image.shape, hand_landmarks_3d
            )
            
            detection_result.update({
                'obstacle_mask': obstacle_mask,
                'inference_time': inference_time,
                'fps': 1.0 / inference_time if inference_time > 0 else 0
            })
            
            return detection_result
            
        except Exception as e:
            print(f"âŒ YOLOv8æ¨ç†å¤±è´¥: {e}")
            return self._get_empty_result()
    
    def _process_detection_results(self, result, image_shape: Tuple[int, int, int]) -> Dict[str, Any]:
        """
        å¤„ç†YOLOv8æ£€æµ‹ç»“æœ
        
        Args:
            result: YOLOv8æ£€æµ‹ç»“æœ
            image_shape: å›¾åƒå½¢çŠ¶ (height, width, channels)
            
        Returns:
            Dict: å¤„ç†åçš„æ£€æµ‹ç»“æœ
        """
        height, width = image_shape[:2]
        
        detection_result = {
            'obstacles': [],
            'hand_regions': [],
            'obstacle_count': 0,
            'hand_region_count': 0,
            'total_detections': 0
        }
        
        if result.masks is None:
            return detection_result
        
        # å¤„ç†æ¯ä¸ªæ£€æµ‹ç»“æœ
        for i, (box, mask, conf, cls) in enumerate(zip(
            result.boxes.xyxy.cpu().numpy(),
            result.masks.data.cpu().numpy(),
            result.boxes.conf.cpu().numpy(),
            result.boxes.cls.cpu().numpy()
        )):
            class_id = int(cls)
            class_name = result.names[class_id]
            confidence = float(conf)
            
            # è°ƒæ•´æ©è†œå¤§å°åˆ°åŸå›¾å°ºå¯¸
            mask_resized = cv.resize(mask, (width, height))
            mask_binary = (mask_resized > 0.5).astype(np.uint8) * 255
            
            detection_info = {
                'id': i,
                'class_id': class_id,
                'class_name': class_name,
                'confidence': confidence,
                'bbox': box.tolist(),
                'mask': mask_binary,
                'area': np.sum(mask_binary > 0)
            }
            
            # åˆ†ç±»ä¸ºéšœç¢ç‰©æˆ–æ‰‹éƒ¨åŒºåŸŸ
            if class_name in self.obstacle_class_names:
                detection_result['obstacles'].append(detection_info)
                detection_result['obstacle_count'] += 1
            elif class_name in self.hand_related_class_names:
                detection_result['hand_regions'].append(detection_info)
                detection_result['hand_region_count'] += 1
            
            detection_result['total_detections'] += 1
        
        return detection_result
    
    def _generate_obstacle_mask(self, 
                               detection_result: Dict[str, Any],
                               image_shape: Tuple[int, int, int],
                               hand_landmarks_3d: Optional[List] = None) -> np.ndarray:
        """
        ç”Ÿæˆéšœç¢ç‰©æ©è†œï¼ˆæ”¹è¿›ç‰ˆæœ¬ï¼Œç¡®ä¿ä¸æ·±åº¦å›¾å¯¹é½ï¼‰
        
        Args:
            detection_result: æ£€æµ‹ç»“æœ
            image_shape: å›¾åƒå½¢çŠ¶
            hand_landmarks_3d: æ‰‹éƒ¨å…³é”®ç‚¹3Dåæ ‡
            
        Returns:
            np.ndarray: éšœç¢ç‰©æ©è†œ
        """
        height, width = image_shape[:2]
        obstacle_mask = np.zeros((height, width), dtype=np.uint8)
        
        # åˆå¹¶æ‰€æœ‰éšœç¢ç‰©æ©è†œ
        for obstacle in detection_result['obstacles']:
            mask = obstacle['mask']
            
            # ç¡®ä¿æ©è†œå°ºå¯¸ä¸å›¾åƒå°ºå¯¸ä¸€è‡´
            if mask.shape != (height, width):
                mask = cv.resize(mask, (width, height))
            
            # äºŒå€¼åŒ–æ©è†œ
            mask_binary = (mask > 128).astype(np.uint8) * 255
            obstacle_mask = cv.bitwise_or(obstacle_mask, mask_binary)
        
        # æ’é™¤æ‰‹éƒ¨åŒºåŸŸ
        obstacle_mask = self._exclude_hand_regions(
            obstacle_mask, detection_result['hand_regions'], hand_landmarks_3d
        )
        
        # åå¤„ç†ï¼šå™ªå£°è¿‡æ»¤å’Œå½¢æ€å­¦æ“ä½œ
        obstacle_mask = self._post_process_mask(obstacle_mask)
        
        return obstacle_mask
    
    def _exclude_hand_regions(self, 
                             obstacle_mask: np.ndarray,
                             hand_regions: List[Dict],
                             hand_landmarks_3d: Optional[List] = None) -> np.ndarray:
        """
        ä»éšœç¢ç‰©æ©è†œä¸­æ’é™¤æ‰‹éƒ¨åŒºåŸŸï¼ˆè°ƒè¯•ç‰ˆæœ¬ï¼‰
        
        Args:
            obstacle_mask: åŸå§‹éšœç¢ç‰©æ©è†œ
            hand_regions: æ‰‹éƒ¨åŒºåŸŸæ£€æµ‹ç»“æœ
            hand_landmarks_3d: æ‰‹éƒ¨å…³é”®ç‚¹3Dåæ ‡
            
        Returns:
            np.ndarray: æ’é™¤æ‰‹éƒ¨åŒºåŸŸåçš„æ©è†œ
        """
        result_mask = obstacle_mask.copy()
        original_pixels = np.sum(obstacle_mask > 0)
        
        # æ’é™¤YOLOv8æ£€æµ‹åˆ°çš„æ‰‹éƒ¨åŒºåŸŸ
        for hand_region in hand_regions:
            hand_mask = hand_region['mask']
            # è†¨èƒ€æ‰‹éƒ¨åŒºåŸŸä»¥ç¡®ä¿å®Œå…¨æ’é™¤
            kernel = np.ones((15, 15), np.uint8)
            hand_mask_dilated = cv.dilate(hand_mask, kernel, iterations=1)
            result_mask = cv.bitwise_and(result_mask, cv.bitwise_not(hand_mask_dilated))
        
        # æ’é™¤MediaPipeæ£€æµ‹åˆ°çš„æ‰‹éƒ¨å…³é”®ç‚¹åŒºåŸŸï¼ˆå‡å°‘è†¨èƒ€ï¼Œé¿å…è¿‡åº¦æ’é™¤ï¼‰
        if hand_landmarks_3d:
            hand_landmark_mask = self._create_hand_landmark_mask(
                hand_landmarks_3d, obstacle_mask.shape
            )
            result_mask = cv.bitwise_and(result_mask, cv.bitwise_not(hand_landmark_mask))
        
        final_pixels = np.sum(result_mask > 0)
        excluded_pixels = original_pixels - final_pixels
        
        # è°ƒè¯•ä¿¡æ¯
        if excluded_pixels > 0:
            print(f"ğŸ” æ‰‹éƒ¨åŒºåŸŸæ’é™¤: {excluded_pixels} åƒç´ è¢«æ’é™¤")
        
        return result_mask
    
    def _create_hand_landmark_mask(self, 
                                  hand_landmarks_3d: List,
                                  mask_shape: Tuple[int, int]) -> np.ndarray:
        """
        åŸºäºæ‰‹éƒ¨å…³é”®ç‚¹åˆ›å»ºæ‰‹éƒ¨åŒºåŸŸæ©è†œï¼ˆå‡å°‘è†¨èƒ€ï¼Œé¿å…è¿‡åº¦æ’é™¤ï¼‰
        
        Args:
            hand_landmarks_3d: æ‰‹éƒ¨å…³é”®ç‚¹3Dåæ ‡åˆ—è¡¨
            mask_shape: æ©è†œå½¢çŠ¶
            
        Returns:
            np.ndarray: æ‰‹éƒ¨åŒºåŸŸæ©è†œ
        """
        height, width = mask_shape
        hand_mask = np.zeros((height, width), dtype=np.uint8)
        
        for landmark in hand_landmarks_3d:
            if len(landmark) >= 2 and landmark[2] > 0:  # æœ‰æ•ˆæ·±åº¦
                x, y = int(landmark[0]), int(landmark[1])
                if 0 <= x < width and 0 <= y < height:
                    # ä¸ºæ¯ä¸ªå…³é”®ç‚¹åˆ›å»ºè¾ƒå°çš„è†¨èƒ€åŒºåŸŸ
                    cv.circle(hand_mask, (x, y), 10, 255, -1)  # å‡å°‘åŠå¾„ä»20åˆ°10
        
        # å‡å°‘è†¨èƒ€ä»¥ç¡®ä¿ä¸ä¼šè¿‡åº¦æ’é™¤éšœç¢ç‰©
        kernel = np.ones((10, 10), np.uint8)  # å‡å°‘æ ¸å¤§å°ä»25åˆ°10
        hand_mask = cv.dilate(hand_mask, kernel, iterations=1)
        
        return hand_mask
    
    def _post_process_mask(self, mask: np.ndarray) -> np.ndarray:
        """
        æ©è†œåå¤„ç†ï¼šå™ªå£°è¿‡æ»¤å’Œå½¢æ€å­¦æ“ä½œ
        
        Args:
            mask: åŸå§‹æ©è†œ
            
        Returns:
            np.ndarray: å¤„ç†åçš„æ©è†œ
        """
        # ç§»é™¤å°çš„å™ªå£°åŒºåŸŸ
        kernel_small = np.ones((3, 3), np.uint8)
        mask = cv.morphologyEx(mask, cv.MORPH_OPEN, kernel_small)
        
        # å¡«å……å°çš„ç©ºæ´
        kernel_medium = np.ones((5, 5), np.uint8)
        mask = cv.morphologyEx(mask, cv.MORPH_CLOSE, kernel_medium)
        
        return mask
    
    def _get_empty_result(self) -> Dict[str, Any]:
        """
        è·å–ç©ºçš„æ£€æµ‹ç»“æœ
        
        Returns:
            Dict: ç©ºç»“æœ
        """
        return {
            'obstacles': [],
            'hand_regions': [],
            'obstacle_count': 0,
            'hand_region_count': 0,
            'total_detections': 0,
            'obstacle_mask': np.zeros((480, 640), dtype=np.uint8),
            'inference_time': 0.0,
            'fps': 0.0
        }
    
    def get_performance_stats(self) -> Dict[str, float]:
        """
        è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            Dict: æ€§èƒ½ç»Ÿè®¡
        """
        if not self.inference_times:
            return {'avg_inference_time': 0.0, 'avg_fps': 0.0, 'max_inference_time': 0.0}
        
        avg_time = np.mean(self.inference_times)
        avg_fps = 1.0 / avg_time if avg_time > 0 else 0.0
        max_time = np.max(self.inference_times)
        
        return {
            'avg_inference_time': avg_time,
            'avg_fps': avg_fps,
            'max_inference_time': max_time,
            'total_inferences': len(self.inference_times)
        }
    
    def visualize_detection(self, 
                           image: np.ndarray, 
                           detection_result: Dict[str, Any]) -> np.ndarray:
        """
        å¯è§†åŒ–æ£€æµ‹ç»“æœ
        
        Args:
            image: åŸå§‹å›¾åƒ
            detection_result: æ£€æµ‹ç»“æœ
            
        Returns:
            np.ndarray: å¯è§†åŒ–å›¾åƒ
        """
        vis_image = image.copy()
        
        # ç»˜åˆ¶éšœç¢ç‰©
        for obstacle in detection_result['obstacles']:
            bbox = obstacle['bbox']
            class_name = obstacle['class_name']
            confidence = obstacle['confidence']
            
            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            x1, y1, x2, y2 = map(int, bbox)
            cv.rectangle(vis_image, (x1, y1), (x2, y2), (0, 0, 255), 2)
            
            # ç»˜åˆ¶æ ‡ç­¾
            label = f"{class_name}: {confidence:.2f}"
            cv.putText(vis_image, label, (x1, y1 - 10), 
                      cv.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)
        
        # ç»˜åˆ¶æ‰‹éƒ¨åŒºåŸŸ
        for hand_region in detection_result['hand_regions']:
            bbox = hand_region['bbox']
            class_name = hand_region['class_name']
            confidence = hand_region['confidence']
            
            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            x1, y1, x2, y2 = map(int, bbox)
            cv.rectangle(vis_image, (x1, y1), (x2, y2), (0, 255, 0), 2)
            
            # ç»˜åˆ¶æ ‡ç­¾
            label = f"{class_name}: {confidence:.2f}"
            cv.putText(vis_image, label, (x1, y1 - 10), 
                      cv.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        stats_text = [
            f"Obstacles: {detection_result['obstacle_count']}",
            f"Hand Regions: {detection_result['hand_region_count']}",
            f"FPS: {detection_result['fps']:.1f}"
        ]
        
        for i, text in enumerate(stats_text):
            cv.putText(vis_image, text, (10, 30 + i * 25), 
                      cv.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        return vis_image
    
    def cleanup(self):
        """
        æ¸…ç†èµ„æº
        """
        if self.model is not None:
            del self.model
            self.model = None
        self.is_initialized = False
        print("âœ… YOLOv8éšœç¢ç‰©æ£€æµ‹å™¨å·²æ¸…ç†")


def main():
    """
    æµ‹è¯•YOLOv8éšœç¢ç‰©æ£€æµ‹å™¨
    """
    print("ğŸš€ æµ‹è¯•YOLOv8éšœç¢ç‰©æ£€æµ‹å™¨...")
    
    # åˆ›å»ºæ£€æµ‹å™¨
    detector = YOLOObstacleDetector(
        model_path="yolov8n-seg.pt",
        confidence_threshold=0.5
    )
    
    if not detector.is_initialized:
        print("âŒ æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥")
        return
    
    # æµ‹è¯•å›¾åƒï¼ˆä½¿ç”¨æ‘„åƒå¤´æˆ–æµ‹è¯•å›¾åƒï¼‰
    cap = cv.VideoCapture(0)
    
    if not cap.isOpened():
        print("âŒ æ— æ³•æ‰“å¼€æ‘„åƒå¤´")
        return
    
    try:
        frame_count = 0
        while frame_count < 100:  # æµ‹è¯•100å¸§
            ret, frame = cap.read()
            if not ret:
                break
            
            # æ£€æµ‹éšœç¢ç‰©
            detection_result = detector.detect_obstacles(frame)
            
            # å¯è§†åŒ–ç»“æœ
            vis_frame = detector.visualize_detection(frame, detection_result)
            
            # æ˜¾ç¤ºéšœç¢ç‰©æ©è†œ
            obstacle_mask = detection_result['obstacle_mask']
            mask_colored = cv.applyColorMap(obstacle_mask, cv.COLORMAP_JET)
            
            # æ˜¾ç¤ºç»“æœ
            cv.imshow('YOLOv8 Obstacle Detection', vis_frame)
            cv.imshow('Obstacle Mask', mask_colored)
            
            frame_count += 1
            if frame_count % 10 == 0:
                stats = detector.get_performance_stats()
                print(f"å¸§ {frame_count}: {stats['avg_fps']:.1f} FPS")
            
            # æŒ‰'q'é€€å‡º
            if cv.waitKey(1) & 0xFF == ord('q'):
                break
    
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­")
    
    finally:
        cap.release()
        cv.destroyAllWindows()
        detector.cleanup()
        print("âœ… æµ‹è¯•å®Œæˆ")


if __name__ == "__main__":
    main()
